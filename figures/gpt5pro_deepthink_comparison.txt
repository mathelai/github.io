(A) Overall performance: Average grades show close performance between the two AI systems. DeepThink achieves 6.44/7 (92.0\%) ahead of GPT-5 Pro at 6.31/7 (90.1\%). Both demonstrate near-mastery of IMO combinatorics. (B) Perfect score rate (grade 7): GPT-5 Pro acheives perfect scores in 87.2\% (34/39 problems) compared with DeepThink's 74.4\% (29/39 problems), a 12.8\% gap. GPT-5 Pro achieves complete solutions more consistently. The high perfect score rates for both systems demonstrate that AI matches or exceeds human expert performance on a majority of IMO combinatorics problems. (C) Failure rate (grade 0): DeepThink has only 5.1\% wrong answers (2/39 problems: imo2010p5, imo2025p6) compared with GPT-5 Pro's 7.7\% (3/39 problems: imo2022p6, imo2024p5, imo2025p6). Both systems failed imo2025p6 which our system succeeded. (D) Performance by contest position: Analysis by problem position (P1-P6) shows systematic patterns. Both systems excel at P4 (second-day opener) with near-perfect performance (GPT-5: 6.57, DeepThink: 6.57 - identical). P3 shows both struggling on this traditionally hard position. P6 (final problem) shows DeepThink performing better (6.29) while GPT-5 Pro struggles more (5.71), suggesting DeepThink handles complex multi-step problems better. P2 and P5 favor GPT-5 Pro (6.71 vs 6.43, and 6.43 vs 6.00), indicating it excels at moderate-difficulty direct problems. P1 performance favors DeepThink (6.57 vs 6.14).
