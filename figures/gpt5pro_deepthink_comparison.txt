Figure: AI Performance on IMO Combinatorics: GPT-5 Pro vs DeepThink

This figure presents a comprehensive comparison of GPT-5 Pro and DeepThink performance on 39 IMO Combinatorics problems (2000-2025), evaluated using strict IMO grading standards where wrong numerical answers receive Grade 0.

Panel (A) - Overall Performance:
Average grades show remarkably close performance between the two AI systems. DeepThink achieves 6.44/7 (92.0%), edging out GPT-5 Pro at 6.31/7 (90.1%) by 0.13 points (1.9% difference). Both systems demonstrate near-mastery of IMO combinatorics, scoring above 90% of maximum possible points. This small but meaningful difference suggests both architectures have reached comparable capabilities on formal mathematical reasoning tasks, representing a significant milestone in AI mathematical problem-solving.

Panel (B) - Perfect Score Rate (Grade 7):
GPT-5 Pro dominates in perfect scores with 87.2% (34/39 problems) compared to DeepThink's 74.4% (29/39 problems) - a 12.8 percentage point gap. This indicates GPT-5 Pro has a higher performance ceiling, achieving complete solutions more consistently. The high perfect score rates for both systems demonstrate that contemporary AI can match or exceed human expert performance on a substantial majority of IMO combinatorics problems. GPT-5 Pro's superior perfect score rate suggests it may have more robust solution strategies or better pattern recognition for standard combinatorial techniques.

Panel (C) - Wrong Answer Rate (Grade 0):
DeepThink shows superior reliability with only 5.1% wrong answers (2/39 problems: imo2010p5, imo2025p6) versus GPT-5 Pro's 7.7% (3/39 problems: imo2022p6, imo2024p5, imo2025p6). Under strict grading where wrong numerical answers receive zero points, this difference proves decisive. DeepThink's lower failure rate indicates more conservative and accurate reasoning, avoiding computational errors and incorrect bounds. Both systems failed imo2025p6, but GPT-5 Pro had additional numerical optimization failures on imo2022p6 (wrong formula) and imo2024p5 (wrong bound).

Panel (D) - Performance by Contest Position:
Analysis by problem position (P1-P6) reveals systematic performance patterns. Both systems excel at P4 (second-day opener) with near-perfect performance (GPT-5: 6.57, DeepThink: 6.57 - identical). P3 shows divergent performance (GPT-5: 6.00, DeepThink: 6.00 after corrected imo2004p3 grade), with both struggling on this traditionally hardest position. P6 (final problem) shows DeepThink performing better (6.29) while GPT-5 Pro struggles more (5.71), suggesting DeepThink handles complex multi-step problems better. P2 and P5 favor GPT-5 Pro (6.71 vs 6.43, and 6.43 vs 6.00), indicating it excels at moderate-difficulty direct problems. P1 performance favors DeepThink (6.57 vs 6.14).

Key Insights for MathEL:
1. **Near-human expert performance**: Both AI systems achieve ~91% accuracy on IMO combinatorics, approaching gold medalist level performance
2. **Reliability advantage**: DeepThink achieves fewer wrong answers (5.1% vs 7.7%), proving decisive under strict grading
3. **Perfect score trade-off**: GPT-5 Pro achieves 87.2% perfect scores vs DeepThink's 74.4%, but this is outweighed by DeepThink's better handling of difficult problems
4. **Position-specific strengths**: DeepThink excels at P1 and P6 (openers and marathon problems); GPT-5 Pro dominates P2/P5 (moderate problems); both tie at P4
5. **P3 crucial difference**: imo2004p3 shows DeepThink Grade 6 (correct 12|mn via chessboard coloring) vs GPT-5 Pro Grade 2 (wrong 6|mn) - this 4-point swing was decisive
6. **Graduated vs bimodal**: DeepThink's 7 Grade 6 problems provide consistent near-complete solutions; GPT-5 Pro shows binary outcomes

Positional Strategy Implications:
- **P1**: Deploy DeepThink-style approaches (6.57 vs 6.14)
- **P2, P5**: Favor GPT-5 Pro strategies (6.71 vs 6.43, 6.43 vs 6.00)
- **P3**: Critical position - DeepThink's updated imo2004p3 proof (Grade 6) shows better handling than GPT-5 Pro (Grade 2)
- **P4**: Both systems perfect; deploy standard approaches (6.57 tie)
- **P6**: Favor DeepThink approaches for sustained reasoning (6.29 vs 5.71)

Implications: MathEL should incorporate DeepThink's reliability mechanisms (2 wrong answers vs 3) and superior handling of difficult problems like imo2004p3 (correct necessary condition 12|mn via chessboard coloring vs GPT-5's wrong 6|mn). The position-specific performance suggests MathEL should adapt its strategy based on problem placement, using GPT-5 Pro-style direct solving for P2/P5 and DeepThink-style sustained reasoning for P1/P6. The complementary strengths indicate ensemble methods could achieve both high perfect score rates AND superior reliability.

Final Verdict: DeepThink wins by 0.13 points (6.44 vs 6.31) under strict grading. The decisive factors are: (1) fewer wrong answers (2 vs 3, giving 5.1% vs 7.7% failure rate), (2) stronger Grade 6 performance (7 vs 1 problems with minor gaps), and (3) crucial 4-point advantage on imo2004p3 where DeepThink was essentially correct while GPT-5 Pro had wrong necessary condition. The positional analysis reveals that success depends on matching the right reasoning approach to problem characteristics, with DeepThink showing better overall reliability.
